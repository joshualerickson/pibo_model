---
title: "Exploring Prediction"
author: "Josh Erickson"
date: "1/27/2022"
output: html_document
---

```{r}
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")),fig.width = 10, fig.height = 5,
                      warning = FALSE, message = FALSE, error = FALSE)

#can change font to whatever, i like Montserrat
thematic::thematic_on(font = 'Montserrat')

# makes the plots sharper
resourceviz::cairo_view()


library(sf)
library(CAST)
library(caret)
library(tidymodels)
library(tidyverse)
library(doParallel)
library(parallel)
library(resourceviz)
```

## Intro  

This is a quick dive into some other features (variables) and methods (models) to predict mean bankfull width using the PIBO dataset and (USGS PROSPER Continuous Parameter Grids (CPG))[https://www.sciencebase.gov/catalog/item/57daf396e4b090824ffc326c]. The reason for using the CPGs is that they are spatially continuous across the landscape, which offers easy transformation of models to raster grid.  

### Findings  

- By using a few key variables, the adjusted $R^2$ goes from 

#### Methods  

We'll do a recursive feature elimination (RFE) to trim down the 31 features on two different models; random forest (rf) and gradient boosting machine (gbm). From there, we'll use glmnet to do regularization on the features without RFE. Thus, we'll end up with three different models that used feature selection techniques. After feature selection we'll use the selected variables on the training data again to tune the models using 10-fold cv. This will give us the final parameters from which we can then use on the test set. Hopefully this will give us an idea of the variables selected and their importance to predicting mean bankfull width as well as model performance, eg r-squared and RMSE. Finally, we'll bring in linear regression from the paper and compare with the models. The idea is to have a better performing model (metrics) while peaking at possible important variables!  

```{r, eval = F, echo=F}
pibo_ml <- read_csv('pibo_ml.csv')

#clean
pibo_ml <- pibo_ml %>% select(1:30, mean_bf,mgmt, MAP_UNIT_N, utm1, utm2) %>% janitor::clean_names() %>% as.data.frame()

cluster10 <- kmeans(pibo_ml[,c('utm1','utm2')], (nrow(pibo_ml)/30))
# add the new variable back to your dataframe here
pibo_ml$spatial_cluster = cluster10$cluster

pibo_ml <- pibo_ml %>% na.omit() %>% mutate(across(is.character, factor))

library(caret)
set.seed(1234)
#split data into training and test data. 75% for training is the goal.
index <- createDataPartition(pibo_ml$mean_bf, list = FALSE, p=0.75)

train <- pibo_ml[index,]

test <- pibo_ml[-index,]

library(CAST)
set.seed(1234)
indices10 <- CreateSpacetimeFolds(train, spacevar = "spatial_cluster", k = 10)

```

```{r, eval=F, echo=F}
#for balanced data
rec_sel <-
  recipe(mean_bf ~ ., data = train) %>%
  update_role(utm1, utm2, spatial_cluster,site_id,  new_role = "bring along") %>%
  step_center(contains("swe_"))  %>%
  step_scale(contains("swe_")) %>%
  step_pca(contains("swe_"), prefix = "pca_swe", threshold = 0.9)%>%
  step_center(contains("hydrogrp"))  %>%
  step_scale(contains("hydrogrp")) %>%
  step_pca(contains("hydrogrp"), prefix = "pca_hydrogrp", threshold = 0.9)


library(doParallel)
library(parallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrl <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all",
  verbose = FALSE,
  allowParallel = TRUE,
  index = indices10$index
)

set.seed(1234)
library(randomForest)
rfe10 <- rfe(rec_sel,
             data = train, metric = "RMSE",
             rfeControl = rfeCtrl,
             sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

train <- as.data.frame(train)

rec_tune <- recipe(mean_bf ~ ., data = train) %>%
  update_role(everything(), -fac_taudem_17all_int,
              -us_precip_1981_2010_cpg_all,-contains('swe_'),
              new_role = "bring along") %>%
  step_center(contains("swe_"))  %>%
  step_scale(contains("swe_")) %>%
  step_pca(contains("swe_"), prefix = "pca_swe", num_comp = 1) %>%
  update_role(spatial_cluster, new_role = 'strata')

set.seed(1234)
library(randomForest)
rf_spec <- rand_forest(mode = 'regression',
                       mtry = tune(),
                       trees = tune(),
                       min_n = tune()) %>%
  set_engine(engine = 'randomForest')

cv_10 <- vfold_cv(train, strata = spatial_cluster)
set.seed(1234)
rf_grid <- tune_grid(rf_spec,
                     rec_tune,
                     resamples = cv_10)
rf_grid %>%
  collect_metrics()
#just show the best
rf_grid %>%
  show_best('rsq')
rf_grid %>%
  select_best('rsq')
rf_wf <- workflow() %>%
  add_recipe(rec_tune) %>%
  add_model(rf_spec)
final_wf <- finalize_workflow(rf_wf,parameters = tibble(mtry = 3,
                                                     trees = 800,
                                                     min_n = 11))
final_cv <- final_wf %>% 
  fit_resamples(cv_10)

```


```{r eval = FALSE, echo=F}

## this just brings in a basic list from which we can then modify
gbmRFE <-  list(summary = defaultSummary,
               fit = function(x, y, first, last, ...){
                 library(randomForest)
                 randomForest(x, y, importance = first, ...)
               },
               pred = function(object, x)  predict(object, x),
               rank = function(object, x, y) {
                 vimp <- varImp(object)
                 vimp <- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
                 vimp$var <- rownames(vimp)                  
                 vimp
               },
               selectSize = pickSizeBest,
               selectVar = pickVars)

#summary function is how are we storing the stats per resample
gbmRFE$summary <- function (data, lev = NULL, model = NULL) 
{
  if (is.character(data$obs)) 
    data$obs <- factor(data$obs, levels = lev)
  postResample(data[, "pred"], data[, "obs"])
}


#now bring in the gbm function and use method = "gbm"
gbmRFE$fit <- function (x, y, first, last, ...) {
  library(gbm)
  caret::train(x, y, method = "gbm")}

#this is the predict function from the "gbm" package
gbmRFE$pred <- function (object, x) 
{
  tmp <- predict(object, x)
  if (object$modelType == "Classification" & object$control$classProbs) {
    out <- cbind(data.frame(pred = tmp), as.data.frame(predict(object, 
                                                               x, type = "prob")))
  }
  else out <- tmp
  out
}

#this is the rank function from the "gbm" package
gbmRFE$rank <- function(object, x, y){
  vimp <- varImp(object, scale = TRUE)$importance
  if (!is.data.frame(vimp)) 
    vimp <- as.data.frame(vimp)
  if (object$modelType == "Regression") {
    vimp <- vimp[order(vimp[, 1], decreasing = TRUE), , drop = FALSE]
  }
  else {
    if (all(levels(y) %in% colnames(vimp)) & !("Overall" %in% 
                                               colnames(vimp))) {
      avImp <- apply(vimp[, levels(y), drop = TRUE], 1, 
                     mean)
      vimp$Overall <- avImp
    }
  }
  vimp <- vimp[order(vimp$Overall, decreasing = TRUE), , drop = FALSE]
  vimp$var <- rownames(vimp)
  vimp
}

#selectSize and selectVar will stay the same

```

```{r, eval=F, echo=F}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrlgbm <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all",
  verbose = FALSE,
  allowParallel = TRUE,
  index = indices10$index
)

set.seed(1234)
library(gbm)
rfe10_gbm <- rfe(rec_sel,
             data = train, metric = "RMSE",
             rfeControl = rfeCtrlgbm,
             sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```

```{r, echo=F, eval = F}
c1 <- ggplot(rfe10_gbm) + ggtitle("Predicting Mean Bankfull Width") +
  theme_bw()
c2 <- xyplot(rfe10_gbm,
             type = c("g", "p", "smooth"),
             ylab = "RMSE")
c3 <- densityplot(rfe10_gbm,
                  subset = Variables %in% c(1:6),
                  adjust = 1.25,
                  as.table = TRUE,
                  xlab = "RMSE CV",
                  pch = "|") 

toltune(rfe10_gbm, 10, "Rsquared", title = "RFE 12th HUC Random Forest")
oneR <- oneSE(rfe10_gbm$results, metric = "Rsquared", num = 10 , maximize = TRUE)

#use oneR approach
selected_var_gbm <- rfe10_gbm$variables %>% filter(Variables == 3)
library(gridExtra)
lay <- rbind(c(1,1,2,2),
             c(0,3,3,0))
grid.arrange(c1,c2,c3,ncol=2, layout_matrix = lay)
```

```{r, echo=F, eval=F}

library(tidymodels)
library(glmnet)

rec_sel <-
  recipe(mean_bf ~ ., data = train) %>%
  update_role(utm1, utm2, spatial_cluster,site_id,  new_role = "bring along") %>% 
  step_center(contains("swe_"))  %>%
  step_scale(contains("swe_")) %>%
  step_pca(contains("swe_"), prefix = "pca_swe", threshold = 0.9)%>%
  step_center(contains("hydrogrp"))  %>%
  step_scale(contains("hydrogrp")) %>%
  step_pca(contains("hydrogrp"), prefix = "pca_hydrogrp", threshold = 0.9) %>%
  step_dummy(all_nominal()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

set.seed(1234)
lr_mod <- 
  linear_reg(penalty = tune(), mixture = 0.5,mode = 'regression') %>% 
  set_engine("glmnet") 
lr_mod
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(rec_sel)

lr_reg_grid <- tibble(penalty = 10^seq(-5, 1, length.out = 30))
cv_10 <- vfold_cv(train, strata = spatial_cluster, v = 10)

lr_res <- 
  lr_workflow %>% 
  tune_grid(grid = lr_reg_grid,
            resamples = cv_10)

lr_res %>% collect_metrics()
autoplot(lr_res)

final_wf <- finalize_workflow(lr_workflow,parameters = tibble(penalty = 0.00001))

final_cv <- final_wf %>% 
  fit_resamples(cv_10)

final_cv %>% collect_metrics()

final_pred <- fit(final_wf, train)

test %>%
  bind_cols(predict(final_pred, test)) %>%
  metrics(mean_bf, .pred)

lr_res %>% 
  pull_workflow_fit() %>% 
  tidy() %>% arrange(desc(estimate))
library(vip)

final_pred %>%
  pull_workflow_fit() %>%
  vip() +
  ggtitle("Elastic Net")



```


```{r}
rfe10

```












---
title: "ANCOVA"
author: "Josh Erickson"
date: "10/26/2021"
output: 
  html_document:
    toc: true
    css: 'style.css'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")),fig.width = 10, fig.height = 5,
                      warning = FALSE, message = FALSE, error = FALSE)
ggplot2::theme_set(ggplot2::theme_bw(base_size = 12))
library(tidyverse)
library(readxl)
library(janitor)
samp_model <- read_csv('samp_model.csv')
```

## TL;DR  

ANCOVA is in the family of General Linear Models (GLM) and is a mix of both linear regression and a t-test/ANOVA. It can help you correct for Type II error by increasing your power through covariates (reducing error unexplained). This is important if you can't control for covariates in the experimental design as it let's you account for this in the analysis albeit sometimes controversial (independence between covariate and IV). It still uses the same fundamentals of NHST (Null Hypothesis Significance Testing) and compares the variance explained to the variance unexplained via the F test. However, it helps *adjust* the means in the F test by decreasing the variance unexplained (Type II error) which in turn increases your F value. Moreover, since it uses both linear regression and t-test/ANOVA you acquire a lot of assumptions along way, which can be unreasonable with most data. But, if your data follows these assumptions then it is a great way to account for Type II error and compare multiple groups.    

## Intro  

In this post we'll go over the *math* behind ANCOVA (Analysis of Covariance) and see how the final output of a function like `sasGM::GLM()` or `car::Anova()` calculates the results. ANCOVA is very similar to ANOVA (Analysis of Variance) and if you want more on that topic please see this [post](https://www.hydroblog.org/blog/anova/anova/2021-08-17/). Just like ANOVA or t-test, ANCOVA requires a continuous dependent variable (DV) and a categorical independent variable (IV) but ANCOVA let's us add some continuous *covariates* to the equation as well. This is the coupled effect of linear regression and t-test/ANOVA. Why might we want to do this? Well, sometimes in experimental design we can't (or it's hard to) control for a random variable but we could measure that variable along the way or glean from the experiment. In other words, we get the variance attributed to these random variables through analysis rather than from experimental design. This can be thought of as 'adjusting' or 'controlling' the means between groups but what you are really doing is to capturing any error between the DV and covariate. This is important because you don't want the error to cross-contaminate, i.e. error is also reducing the IV. The goal is to see how and why this happens. Below we will try and learn about these topics,  

* Quick overview of t-test/linear regression  
* Start adding covariates
* Walk through results

My goal is to add as much visualization as possible as I think that will help capture the interplay between linear regression and ANCOVA. I'll also skim over some of the assumptions and won't go into too much detail albeit **very very important**.  

### t-test and Linear Regression

Why the heck are we going to talk about a t-test and linear regression for ANCOVA? Well, they are both within ANCOVA and since that's what we'll be building up to I think it will help us to understand ANCOVA a little better (IMO). First we'll go over linear regression and then show how that is really similar to a t-test.  

#### Linear Regression  

Linear regression is a common technique to use when you want to understand a relationship between a DV and a covariate; however, it needs to follow certain assumptions: must be linear, normality of residuals, iid, homoscedastic, multicollinearity (more than one covariate). As you can see, we are taking on a lot of assumptions with linear regression but remember if your data doesn't violate these then it is very realistic test to use (low variance and interpretable)! Let's bring in some data and start with a simple linear regression example. We'll be looking at PIBO surveys with different metrics (channel geometry and zonal stats). We want to see if there is a relationship between watershed area (covariate) and mean bankfull width (DV).  

#### Data  Visualizing  

Let's plot the data and see what's going on. We would guess that if you increase your drainage area you would increase the mean bankfull width due to bigger drainage areas being bigger thus leading to higher bankfull widths (more stream power) in response reaches (low gradient (0-4%) and dominant gravel/cobble).  


```{r, echo=F}

gr_95_all <- read_xlsx('C:/Users/joshualerickson/Box/PIBO and Water Yield/Data/R1WestDividePIBO.xlsx',
                       sheet = 'R1WestHabitat') %>% clean_names()


gr_95_new <- read_xlsx('C:/Users/joshualerickson/Box/PIBO and Water Yield/Data/R1WestPIBO110521.xlsx') %>% 
  clean_names()

gr_95 <- read_csv('SummaryStatsKendall_gr_95.csv')[,1:36] %>% 
  na.omit() %>%
  clean_names() %>% 
  right_join(gr_95_new %>% select(site_id), by = 'site_id') %>%
  mutate(precip_cut = cut_interval(ave_annual_precip,n=15),
         shed_cut = cut_interval(shed_areakm2, n = 15),
         mgmt = factor(mgmt)) %>% 
  mutate(mean_bf_ft = mean_bf*3.28084,
         shed_areami2 = shed_areakm2*0.386102,
         precip_in = ave_annual_precip*0.0393701)

gr_95_ts <- gr_95_all %>% 
  right_join(gr_95 %>% select(site_id, drain_density:pct_nfs, mean_bf:shed_cut, -mgmt)) %>% 
  mutate(bf_ft = bf*3.28084)

bf_sd_ts <- gr_95_ts %>% group_by(site_id, mgmt) %>% 
  summarise(bf_sd = sd(bf, na.rm = T))%>% 
  left_join(gr_95 %>% select(site_id, drain_density:pct_nfs,stream,
                             mgmt,mean_bf:precip_in) %>% group_by(site_id) %>% 
              slice(n=1), by = c('site_id', 'mgmt')) %>% ungroup()%>% 
  mutate(impact = if_else(mgmt == 'Reference', pct_mtbs_mod+pct_mtbs_high, 
                          pct_high_harv_int+pct_mod_harv_int),
         bf_sd_bc = car::bcPower(bf_sd, 0),
         bf_sd_scale = scale(bf_sd))


```

```{r, echo=FALSE}
library(ggtext)
bf_sd_ts %>% 
  ggplot(aes(shed_areami2, mean_bf_ft)) +
  geom_point() +
  geom_smooth(method = 'lm', se = F) +
  labs(x = 'Watershed Area (mi<sup>2</sup>)',
       y = 'Mean Bankfull Width (ft)',
       title = 'Scatterplot of DV and IV',
       subtitle = 'line is the best fit (OLS)') +
  theme(axis.title.y = element_markdown(),
        axis.title.x = element_markdown())
```


As you can see from the plot above it looks like it's worthy of adding watershed area but how can we measure that? Like, how could we say that it's worth adding? Well, there's is a nice way to test this with an F test. We'll be focusing on the F test because it'll relate to the t-test/ANOVA and is a common way to compare two different models. The F test basically compares the variance explained (model 1 & model 2) by the variance unexplained (model 2) in the DV. This is so close to $R^2$ but I won't get distracted... What we'll do is walk through this and solve for the equation below. Let's look at the F test equation below.  
$$
{\displaystyle F=\frac{\text{explained variance}}{\text{unexplained variance}}= \frac{\sum_{i=1}^{K}n_{i}({\bar{Y}}_{i\cdot}-{\bar {Y}})^{2}/(K-1)}{\sum _{i=1}^{K}\sum _{j=1}^{n_{i}}\left(Y_{ij}-{\bar {Y}}_{i\cdot }\right)^{2}/(N-K)}}
$$

Ok, too much? Yes, too much! Let's start with the numerator and work through trying to understand why we are saying *explained variance*. The numerator is essentially the variance that we can explain from both the DV and the fit. Let's break this up into two ideas: sum of the squares of the DV **SS(mean)** or (model 1) and sum of the squares of the fit **SS(fit)** or (model 2). Let's visualize what these would look like with our data.  

```{r, echo=F}
mean_of_DV <- mean(bf_sd_ts$mean_bf_ft)

bf_sd_ts %>% 
  ggplot(aes(shed_areami2, mean_bf_ft)) + 
  geom_point() +
  geom_segment(aes(xend = shed_areami2, yend = mean_of_DV), linetype = 2,size = 0.25)+
  geom_hline(yintercept = mean_of_DV, size = 1.5, color = 'red')+
  labs(x = 'Watershed Area (mi<sup>2</sup>)',
       y = 'Mean Bankfull Width (ft)',
       title = 'Looking at SS(mean) of data',
       subtitle = 'summing up distance from point to red line')+
  theme(axis.title.y = element_markdown(),
        axis.title.x = element_markdown())
```
This is what the equation would look like below, where $DV_i$ are just individual observations from $i$ to $n$ and $\overline{DV}$ is just the mean of the DV.  

$$
{\displaystyle \text{SS(mean)} = \sum_{i=1}^{n}{(DV_i-\overline{DV})^2}}
$$
In the graph and equation above you can see that we are just taking all of the points and finding their distance to the overall DV mean, squaring, then summing them up. This is essentially a *model* of the DV, right? Yes! Remember we want to see if adding another variable to the DV is useful or not so we need to see what the model does on it's own. 

Pretty simple, right? Now what would the **SS(fit)** look like? Well below you see we just do the same thing as above except now we take the distance to the fit line. This is what adding a variable does to the variance and what we want to compare!


```{r, echo = F}
linear_model <- lm(mean_bf_ft~shed_areami2, data = bf_sd_ts)
pred_model <- predict(linear_model)
bf_sd_ts$pred_lm <- pred_model
bf_sd_ts %>% 
  ggplot(aes(shed_areami2, mean_bf_ft)) + 
  geom_point() +
  geom_segment(aes(xend = shed_areami2, yend = pred_lm), linetype = 2,size = 0.25)+
  geom_smooth(method = 'lm', size = 1.5, color = 'red', se = F)+
  labs(x = 'Watershed Area (mi<sup>2</sup>)',
       y = 'Mean Bankfull Width (ft)',
       title = 'Looking at SS(fit) of data',
       subtitle = 'summing up distance from point to red line')+
  theme(axis.title.y = element_markdown(),
        axis.title.x = element_markdown())
```

Here's what the equation would look like.

$$
{\displaystyle \text{SS(fit)} = \sum_{i=1}^{n}{(y_i-\hat{y})^2}}
$$
Now if we take these two equations and subtract the error of the fit from the error of the mean we would get the explained error right? Yes, well we need to account for parameters and degrees of freedom ($K$ and $N$) but essentially that is what we're doing. Now we need to solve for the unexplained variance in the denominator. Well, lucky for us that is **SS(fit)**! So, now let's bring it all together.  
$$
{\displaystyle F= \frac{\sum_{i=1}^{K}n_{i}({\bar{Y}}_{i\cdot}-{\bar {Y}})^{2}/(K-1)}{\sum _{i=1}^{K}\sum _{j=1}^{n_{i}}\left(Y_{ij}-{\bar {Y}}_{i\cdot }\right)^{2}/(N-K)}=\frac{\text{SS(mean)}-\text{SS(fit)}/(K-1)}{\text{SS(fit)/(N-K)}}}
$$


So now let's solve for F.  

```{r}

# Just find SS(mean)
ss_mean_lr <- bf_sd_ts %>% mutate(overall_mean = mean_of_DV) %>% 
  summarise(ssa = sum((mean_bf_ft - overall_mean)^2))

# Now solve for SS(fit)
ss_fit_lr <- bf_sd_ts %>% mutate(overall_mean = mean_of_DV) %>%
  summarise(sse = sum((mean_bf_ft - pred_lm)^2)) 

# K = 2 since we only have two parameters; intercept and beta

# Numerator
msa_lr <- (ss_mean_lr$ssa-ss_fit_lr$sse)/(2-1)

# Denominator
mse_lr <- ss_fit_lr$sse/(nrow(bf_sd_ts)-2)

# Now find F

f_lr <- msa_lr/mse_lr

```

We get an F score of `r round(f_lr,2)`, which is what we get if we run the `lm()` below.  
```{r}
# Using R function to find result
broom::glance(lm(mean_bf_ft~shed_areami2, data = bf_sd_ts))$statistic
```

This is nice because we can test against the F-distribution, which looks like the figure below. An F-score value at a significance level of 0.05 is (using same degrees of freedom) `r round(qf(0.95,2,214),3)` which we can then test to see if our F-score is larger. Thus, either rejecting or accepting the null. We had a large F-score so we can confidently say that the the variation explained by watershed area was significantly larger the the variation not explained by watershed area.

```{r, echo=F}
ggplot(data = tibble(f = rf(n = 10000, 214,2)), aes(f)) +
  geom_density() + xlim(0,10) + 
  geom_vline(xintercept = 3.038, color = 'red') +
  labs(x = 'F-score', title = 'F distribution showing were 0.05 would be',
       subtitle = paste('we had a large F-score (', round(broom::glance(lm(mean_bf_ft~shed_areami2, data = bf_sd_ts))$statistic, 2),') so we can reject the null!'))
```

Which makes sense right? If you had high variation in SS(fit) (high value) then the numerator would be a small number resulting in a low F-score! So the better the *fit*, then the better the F-score will be. Now that we got linear regression out of the way, let's move on to t-tests.  

#### t-tests  

With a t-test we want to see if there is a difference between groups. Side-bar, if you just have two groups you use a t-test and if you have more than two you use ANOVA (assuming parametric/normal). This is helpful because we wouldn't be able to do the same analysis as above because now we have a categorical variable. However, it's pretty much the same idea. We want to see if the variance explained by both groups is much different than the overall mean. We are going to use the same equation as above in linear regression but now we are going to add a grouping variable called management with two levels: reference and managed. We want to see if there is a difference between reference and managed mean bankfull widths. So let's dive in. Just as above remember we split it into bite size pieces: $\text{SS(mean)}$ and $\text{SS(fit)}$.

$$
{\displaystyle F= \frac{\text{SS(mean)}-\text{SS(fit)}/(K-1)}{\text{SS(fit)/(N-K)}}}
$$
Now we just need to solve for these pieces SS(mean) and SS(fit). Let's visualize SS(mean).  

```{r, echo=F}

bf_sd_ts %>% 
  ggplot(aes(mgmt, mean_bf_ft, color = mgmt)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.3)+
  geom_segment(aes(xend = mgmt, yend = mean_of_DV),
               position = position_jitter(width = 0.05), linetype = 2,size = 0.25)+
  geom_hline(yintercept = mean_of_DV, size = 1.5, color = 'red') +
  guides(colour = guide_legend(override.aes = list(alpha = 1)))+
  annotate("text",
           x = c(1.5),
           y = c(30),
           label = c('overall group mean'),
           family = "", size=4) +
  labs(x = 'Group',
       y = 'Mean Bankfull Width (ft)',
       color = 'Group',
       title = 'Looking at SS(mean) of data',
       subtitle = 'summing up distance from points to red line')
```

Same as before we just sum up all the residuals (distance from point to overall mean) after squaring and get SS(mean). Pretty cool, huh?! Now let's see what the SS(fit) will do since we don't have a linear combination. All it does is take each mean separately for each group and then sum the residuals per group after squaring. That's it! Let visualize SS(fit).  

```{r, echo = F}

bf_sd_ts %>% 
  ggplot(aes(mgmt, mean_bf_ft, color = mgmt)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.3)+
  geom_segment(aes(xend = mgmt, yend = mean_of_DV),
               position = position_jitter(width = 0.05), linetype = 2,size = 0.25)+
  geom_segment(aes(x = .75, xend = 1.25, y = mean(bf_sd_ts[bf_sd_ts$mgmt == 'Managed',]$mean_bf_ft), yend = mean(bf_sd_ts[bf_sd_ts$mgmt == 'Managed',]$mean_bf_ft)), size = 2, color = '#F8766D')+
  geom_segment(aes(x = 1.75, xend = 2.25, y = mean(bf_sd_ts[bf_sd_ts$mgmt == 'Reference',]$mean_bf_ft), yend = mean(bf_sd_ts[bf_sd_ts$mgmt == 'Reference',]$mean_bf_ft)),size = 2, color = '#00BFC4')+
  guides(colour = guide_legend(override.aes = list(alpha = 1)))+
  labs(x = 'Group',
       y = 'Mean Bankfull Width (ft)',
       title = 'Looking at SS(fit) of data',
       color = 'Group',
       subtitle = 'summing up distance from points to each groups mean')

```


So now let's solve for F.  

```{r}

# need to get mean of each group for SS(fit) and SS(mean)
mean_of_each <- bf_sd_ts %>% 
  group_by(mgmt) %>% summarise(mean_of_each = mean(mean_bf_ft)) %>% ungroup


ss_mean_t <- bf_sd_ts %>% mutate(overall_mean = mean_of_DV) %>% 
  summarise(ssa = sum((mean_bf_ft - overall_mean)^2))


ss_fit_t <- bf_sd_ts %>% mutate(overall_mean = mean_of_DV) %>% 
  left_join(mean_of_each, by = 'mgmt') %>% 
  group_by(mgmt) %>% summarise(sse = sum((mean_bf_ft - mean_of_each)^2)) %>% 
  pull(sse) %>% sum()

# K = 2 since we only have two parameters; intercept and beta

msa_t <- (ss_mean_t$ssa-ss_fit_t)/(2-1)
mse_t <- ss_fit_t/(nrow(bf_sd_ts)-2)

f_t <- msa_t/mse_t

```

We get an F score of `r round(f_t,2)`, which is what we get if we run the `lm()` below.  
```{r}
model <- lm(mean_bf_ft~mgmt, data = bf_sd_ts)

broom::tidy(anova(model))[1,]$statistic
```

This tells us that there is a statistical significant difference between groups at an alpha of 0.05 since $F_{(0.05)}(2,214)=3.885$ is greater than our F-score of `r round(broom::tidy(anova(model))[1,]$statistic,2)` thus accepting the null of our hypothesis test. I think it's a good time to talk about a p value real quick and how it **_only_** relates to our test! A p value is the probability, under the assumption that there is no true effect or no true difference, of collecting data that shows a difference equal to or more extreme than what you actually *observed*. That's it! It doesn't tell us the probability of seeing this effect if we repeated the sample over and over again (this is more power than p value) or the probability our hypothesis is true or false (baserate fallacy) it just tells us whether to be surprised or not given our sample. I think this is important because if we just took this as *truth* (the p value/F test) then you'll see in the next section that were are actually not accounting for all of the error. This is the whole idea of modeling! We want to get our model as deterministic as possible, which means we need to account for as much error as possible. So in the next section we'll start accounting for other factors (error) like watershed area and precipitation in our analysis. This is why ANCOVA helps with Type II error because we start bringing in more and more potential explained error or in some cases unexplained.   

:::{.b--red .ba .bw2 .ma2 .pa4 .shadow-1}

I just briefly touched on this at the beginning but it's very important; cross-contamination. This is where we need to be cautious about the covariates we want to use. This is laid out in Miller & Chapman (2001) but is really important because you can increase your chances of misleading results when your IV isn't independent of the covariate. In our situation this doesn't seem to be an issue (precipitation and area were there before experiment and not affected by treatment) but in others it can lead to misleading results. I'm sure this never happens to any of us `r emo::ji('wink')`.  

:::

### ANCOVA  

So we started with linear regression and then talked about t-tests so let's bring those together and do ANCOVA! I think the best way to understand ANCOVA is to think about what both linear regression and t-test were doing. Remember if the $\text{SS(fit)}$ explains more of the variance (low value) then your F result will be high since the denominator is $\text{SS(fit)}$. The same is true for ANCOVA but with ANCOVA you get to adjust for the error by combining both linear regression and t-test.   

```{r}
model_ancova <- lm(mean_bf_ft~shed_areami2+mgmt,data = bf_sd_ts)

pred_ancova <- predict(model_ancova)
```

So as you can see in the equation above we are just using linear regression with additive factor variable (`mgmt`). This will be our $\text{SS(fit)}$. So if we were to visualize this it would look like the graph below where the residuals are adjusted by the watershed area, i.e. the model takes into account whether the observation is 'Managed' or 'Reference'.  

```{r, echo = FALSE}
bf_sd_ts$pred_ancova <- pred_ancova
bf_sd_ts %>% 
  ggplot(aes(shed_areami2, mean_bf_ft,color = mgmt)) + 
  geom_point() +
  geom_line(aes(y = pred_ancova), size = 0.5) +
  geom_segment(aes(xend = shed_areami2,yend = pred_ancova), linetype = 2,size = 0.5)+
  labs(x = 'Watershed Area (mi<sup>2</sup>)',
       y = 'Mean Bankfull Width (ft)',
       title = 'Looking at SS(fit) of data',
       subtitle = 'summing up distance from group to associated line',
       color = 'Group')+
  theme(axis.title.y = element_markdown(),
        axis.title.x = element_markdown())

```

Now that we got our $\text{SS(fit)}$ let's solve for the $\text{SS(mean)}$ of our grouping factor but it doesn't *have* to be called that (remember the F-test is just comparing model 1 to model 2) so I think it's better to look at it as Model 1 vs Model 2, i.e. the one with the covariate and the one without. If the covariate we added helped then it should drop our p value (Type II error), i.e. increase the F value. What we are essentially doing is seeing what's left over (error) after accounting for the covariate. Let's look and see what's going on. We'll take the SS(fit) from the linear regression (Model 1) and subtract the SS(fit) from the new model (Model 2). This should give us the difference between the two. Which makes sense right? You can look at it a couple different ways but we really are just seeing if a model with no covariate is better than a model with a covariate (similar to both the linear regression and t-test examples above). In the graphs below you can see we are essentially doing this, e.g. take linear model (Model 1) or $\text{SS(mean)}$ and subtract overall (new model) $\text{SS(fit)}$. 

Remember,  

$$
{\displaystyle F= \frac{\text{SS(mean)}-\text{SS(fit)}/(K-1)}{\text{SS(fit)/(N-K)}}=\frac{\text{Model 1}-\text{Model 2}/(K-1)}{\text{Model 2/(N-K)}}} 
$$
```{r, echo=F}
library(latex2exp)
p1 <- bf_sd_ts %>% 
  ggplot(aes(shed_areami2, mean_bf_ft)) + 
  geom_point() +
  geom_smooth(method = 'lm', se = F, color = 'black', size = 1)+
  theme_void() +
  labs(title = 'Model 1', subtitle = 'Y = 	&beta;<sub>0</sub> + &beta;<sub>1</sub>*Watershed Area + &epsilon;')  +
  theme(plot.subtitle = element_markdown())

p_plus <- data.frame(x = seq(0,10,1), y = seq(0,10,1)) %>% 
  ggplot(aes(x, y)) + 
  geom_segment(aes(x = 5, xend = 6, y = 6, yend = 6), size = 6) +
  geom_text(aes(x = 5.5, y = 6, label = 'Subtract'), nudge_y = .15,
           family = "", size=4)+
  xlim(c(4,7))+ ylim(c(5,7)) +
  theme_void()

p2 <- bf_sd_ts %>% 
  ggplot(aes(shed_areami2, mean_bf_ft)) + 
  geom_point(aes(color = mgmt)) +
  geom_line(aes(y = pred_ancova, color = mgmt), size = 1)  +
  theme_void() + labs(color = 'Groups',title = 'Model 2', subtitle = 'Y = 	&beta;<sub>0</sub> + &beta;<sub>1</sub>*Watershed Area + &beta;<sub>2</sub>*Group + &epsilon;')  +
  theme(plot.subtitle = element_markdown())

library(patchwork)
p1|p_plus|p2

```

Below we can start to unpack this in our trusty F test.   

```{r, eval = T}
# get residuals from fit, e.g. Model 2
ss_fit <- deviance(model_ancova)

#now find linear regression fit (Model 1) - Model 2.
ss_mean_mgmt <- ss_fit_lr$sse-ss_fit

#now solve for the F value
f_ancova_mgmt <- (ss_mean_mgmt)/(ss_fit/(nrow(bf_sd_ts)-3))

cat('Calculated f statistic', f_ancova_mgmt)

cat('R function f statistic', broom::tidy(car::Anova(lm(mean_bf_ft~shed_areami2+mgmt, data = bf_sd_ts), type = 'III'))[3,4]$statistic)

```

The linear regression fit (Model 1) by itself is not as good when compared to adding an IV. Remember, models want to find the most deterministic way of accounting for error (&epsilon; in equations) and ANCOVA does that with covariates and an IV(s). We went from an F value of 10.085 to 15.077 just by adding the watershed area. So we're done, right? Nope! What about precipitation? We know that area is an effect on mean bankfull width but wouldn't you think precipitation would be as well? Let's check out and see if there is a correlation or not. Same as above we'll compare a model with and without the group. As you can see below our p value is now up at 0.1132! What happened? Well what this is telling us is that it's not any better to add `mgmt` when comparing to the original model (precipitation and watershed area). All this means is that `mgmt` is now not the best when accounting for more variance (error), that's all. 


```{r, eval=T}
model_ancova <- lm(mean_bf_ft~precip_in+shed_areami2+mgmt,data = bf_sd_ts)
model_norm <- lm(mean_bf_ft~precip_in+shed_areami2,data = bf_sd_ts)
anova(model_ancova, model_norm)
```

We can see below we'd get the same result if we ran the ANCOVA using `sasLM::GLM()` and `car::Anova()`. In the readout the `Sum Sq` is just our individual fitting error aka $\text{SS(mean)-SS(fit)}$ or Model 1 - Model 2 for each variable and the residuals is the $\text{SS(fit)}$ or Model 2! We use the F test to calculate the F scores and the F distribution to find the probability of equalling or exceeding.  

```{r}
#don't really need type III here but just showing. 
car::Anova(lm(mean_bf_ft~shed_areami2+precip_in+mgmt, data = bf_sd_ts), type = 'III')
sasLM::GLM(mean_bf_ft~shed_areami2+precip_in+mgmt, Data = bf_sd_ts)[["Type III"]]
```

This all boils down to the assumptions being met and the F test being the analysis of choice. It's all about making/accounting for the explained variance between the covariate and the dependent variable. Sometimes you can get fooled though by overfitting, violation of assumptions, spurious correlation, etc but it helps to see **where** that might make a difference, hint-hint the covariate(s) in the F test, e.g. $\text{SS(mean)-SS(fit)}$.  
<center>
**F test**
</center>
$$
{\displaystyle F= \frac{\text{SS(mean)}-\text{SS(fit)}/(K-1)}{\text{SS(fit)/(N-K)}}}
$$

#### more?  

This is where we need to be careful... A model only does what you tell it to do and also assumes everything is good to go, e.g. no assumptions violated! This is important because we are assuming that we've got a proper sample from a population (randomized) and that we can explain or not explain the variance, right? I'll illustrate below how this can seem uneasy when you set up an experimental design. Let's randomly sample the data we have so that each group has 80 samples and then do ANCOVA like above; however, we'll repeat this 10,000 times. This will give us an idea of what it would look like if we repeated this experiment over and over again, right?  


```{r, echo = F}
samp_model %>% 
  ggplot(aes(p.value)) + 
  geom_histogram(binwidth = 0.01) + 
  scale_x_log10(labels = scales::comma_format(accuracy = 0.001)) + 
  geom_vline(xintercept = 0.05, color = 'red', size = 1) +
  annotate('text', x = 0.03, y = 75, label = '0.05 cutoff') + 
  labs(y = 'count', title = 'Histogram showing ANCOVA repeated for Y = 	&beta;<sub>0</sub> + 	&beta;<sub>1</sub>*Precip + &beta;<sub>2</sub>*Watershed Area + &beta;<sub>3</sub>*Group',
         subtitle = '10,000 iterations of randomly sampled groups (80 per group)')+
  theme(axis.title.y = element_markdown(),
        axis.title.x = element_markdown(),
        plot.title = element_markdown())
```

As you can see from the graph above we get a lot of mixed results. For a p value less than 0.05 we get around `r round(nrow(samp_model %>% filter(p.value <= 0.05))/nrow(samp_model), 4)*100`% of the results. So why is this important? Well it shows us just how finicky covariates and IV's can be based on a random sample but also tells us that we might not be too far off on our interpretation of a 0.1132 p value.  

## Conclusion  

This all hinges on whether or not we can justify or follow the assumptions in the model! However, it does give you an idea of how the final p values are calculated and maybe shed a little more light into the nuts and bolts of ANCOVA. In this example, it really is just comparing two different models and seeing if one is *statistically* different than the other. Hopefully this helps when reading through the results of functions like `sasLM::GLM()` and `car::Anova()`.  

## References  

Miller, G. A., & Chapman, J. P. (2001). Misunderstanding analysis of covariance. Journal of abnormal psychology, 110(1), 40.


































